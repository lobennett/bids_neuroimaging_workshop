{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "from nilearn import plotting, maskers, glm, image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from bids import BIDSLayout\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn.glm.first_level import hemodynamic_models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Local application imports\n",
    "from utils.spm_hrf import spm_hrf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000cf5",
   "metadata": {},
   "source": [
    "### Data organization\n",
    "\n",
    "The data are stored using a framework called the [Brain Imaging Data Structure](https://bids.neuroimaging.io/) (BIDS), which provides a standard way to organize neuroimaging data. These data can then be queried using the [PyBIDS](https://pypi.org/project/pybids/) Python package.\n",
    "\n",
    "We will use the dataset from Haxby et al., 2001, which is available from OpenNeuro [ds000105](https://openneuro.org/datasets/ds000105/versions/3.0.0). Here are details about the data (from [PvMVPA](http://www.pymvpa.org/datadb/haxby2001.html)):\n",
    "\n",
    ">This is a block-design fMRI dataset from a study on face and object representation in human ventral temporal cortex. It consists of 6 subjects with 12 runs per subject. In each run, the subjects passively viewed greyscale images of eight object categories, grouped in 24s blocks separated by rest periods. Each image was shown for 500ms and was followed by a 1500ms inter-stimulus interval. Full-brain fMRI data were recorded with a volume repetition time of 2.5s, thus, a stimulus block was covered by roughly 9 volumes. This dataset has been repeatedly reanalyzed. For a complete description of the experimental design, fMRI acquisition parameters, and previously obtained results see the references below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e959c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data in PyBIDS layout that was installed from download.sh\n",
    "bids_dir = Path(\"./ds000105\")\n",
    "layout = BIDSLayout(bids_dir, derivatives=True)\n",
    "print(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2450b9",
   "metadata": {},
   "source": [
    "First we query for the anatomical image, which is used for overlaying the functional imaging data, and also to align individual brains to a common template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c27a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_imgs = layout.get(\n",
    "    subject=1, extension=\"nii.gz\", suffix=\"T1w\", return_type=\"filename\"\n",
    ")\n",
    "\n",
    "print(\"Found the following anatomical images for sub-1:\")\n",
    "for img in anat_imgs:\n",
    "    print(img)\n",
    "\n",
    "# Now let's load the anatomical image and display the header\n",
    "anat_img_nib = nib.load(anat_imgs[1])\n",
    "print(\"The first anatomical image has the following header information:\\n\")\n",
    "print(anat_img_nib.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba640d90",
   "metadata": {},
   "source": [
    "These data are stored in a common image format known as [NifTI](https://nifti.nimh.nih.gov/nifti-1/), which is the main standard used for neuroimaging data in our field. \n",
    "\n",
    "We can display the anatomical image using the Nilearn plotting tools (see nilearn documentation for [plot_anat](https://nilearn.github.io/dev/modules/generated/plotting.plot_anat.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9df66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vmax to 200 to provide better contrast\n",
    "plotting.plot_anat(anat_imgs[1], vmax=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_imgs = layout.get(\n",
    "    subject=1, extension=\"nii.gz\", suffix=\"bold\", return_type=\"filename\"\n",
    ")\n",
    "\n",
    "print(\"Found the following functional images for sub-1:\")\n",
    "for img in func_imgs:\n",
    "    print(img)\n",
    "\n",
    "# These images are 4-dimensional.\n",
    "# We can load their data using the nibabel package (https://pypi.org/project/nibabel/) and inspect its contents:\n",
    "func_img = nib.load(func_imgs[0])\n",
    "func_data = func_img.get_fdata()\n",
    "\n",
    "print(f\"Functional image has shape: {func_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c5c57",
   "metadata": {},
   "source": [
    "The data comprise 121 3D images. The three dimensions refer to left/right, up/down, and front/back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(image.index_img(func_imgs[0], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a4c22",
   "metadata": {},
   "source": [
    "### Task information\n",
    "In order to analyze the fMRI data we need to also load the information regarding which stimuli were presented at which times. These are stored according to the BIDS standard in an \"events.tsv\" associated with each task run. The standard format has three columns that specify the onset of each event (in seconds from the beginning of the run), the duration of the event (in seconds), and a variable called trial_type that specifies the experimental condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7343fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = layout.get(\n",
    "    subject=1, extension=\"tsv\", suffix=\"events\", return_type=\"filename\"\n",
    ")\n",
    "\n",
    "events = {}\n",
    "for f in event_files:\n",
    "    run_num = f.split(\"_\")[-2]\n",
    "    events[run_num] = pd.read_csv(f, sep=\"\\t\")\n",
    "\n",
    "print(f\"Found {len(events)} event files\")\n",
    "events[run_num].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db470739",
   "metadata": {},
   "source": [
    "There should be 8 different object types presented in each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = events[run_num].trial_type.unique()\n",
    "print(conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab0fd4",
   "metadata": {},
   "source": [
    "### Analyzing the data using the general linear model (GLM)\n",
    "\n",
    "The standard approach to anlyzing task fMRI data is to model each experimental condition as a regressor in a general linear model.\n",
    "\n",
    "Let's create a design matrix that specifies these regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about timeseries length\n",
    "n_timepoints = func_img.dataobj.shape[3]\n",
    "print(n_timepoints, \"timepoints\")\n",
    "\n",
    "# Get the length of each image acquisition, known as repetition time or TR\n",
    "TR = func_img.header.get_zooms()[3]\n",
    "print(\"TR:\", TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6f22f",
   "metadata": {},
   "source": [
    "We will now create the design matrix for each run, saving each to a dictionary indexed by the run label. We will actually create two design matrices: one is an untransformed representation of the task as a boxcar (with ones during task blocks for each condition and zeros otherwise). We will discuss the second design matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "desmat = {}\n",
    "desmat_conv = {}\n",
    "\n",
    "# onset times for each timepoint, in seconds\n",
    "frame_times = np.arange(0, n_timepoints * TR, TR)\n",
    "\n",
    "# length of each task block, in seconds\n",
    "block_length = 24\n",
    "\n",
    "for run_num, event_df in events.items():\n",
    "    # first create empty design matrices, to be filled below\n",
    "    desmat[run_num] = np.zeros((n_timepoints, len(conditions)))\n",
    "    desmat_conv[run_num] = np.zeros(desmat[run_num].shape)\n",
    "\n",
    "    for i, cond in enumerate(conditions):\n",
    "        # create a single block instead of using individual events\n",
    "        cond_events = events[run_num].query(f'trial_type == \"{cond}\"')[\n",
    "            [\"onset\", \"duration\"]\n",
    "        ]\n",
    "        cond_events[\"amplitude\"] = 1\n",
    "\n",
    "        # set the first event to the block length, and then remove the rest\n",
    "        cond_events.iloc[0, 1] = block_length\n",
    "        cond_events = cond_events.iloc[0, :]\n",
    "\n",
    "        # create the unconvolved design matrix\n",
    "        desmat[run_num][:, i] = hemodynamic_models.compute_regressor(\n",
    "            cond_events.to_numpy()[:, np.newaxis], None, frame_times, oversampling=50\n",
    "        )[0][:, 0]\n",
    "\n",
    "        # create the design matrix convolved with the SPM hemodynamic response\n",
    "        desmat_conv[run_num][:, i] = hemodynamic_models.compute_regressor(\n",
    "            cond_events.to_numpy()[:, np.newaxis], \"spm\", frame_times, oversampling=50\n",
    "        )[0][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b5419",
   "metadata": {},
   "source": [
    "It is customary to display the design matrix as a image, with timepoints on the vertical axis and conditions on the horizontal axis. In this case, the white blocks represent ones and the black areas represent zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(desmat[run_num], aspect=\"auto\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf50865d",
   "metadata": {},
   "source": [
    "We can also plot these as timeseries, which shows that each has a boxcar form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(frame_times, desmat[run_num])\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.legend(conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f6cce",
   "metadata": {},
   "source": [
    "### Hemodynamic convolution\n",
    "\n",
    "The standard fMRI technique (known as Blood Oxygen Level Dependent or BOLD fMRI) measures changes in MRI signal related to changes in blood oxygen levels, which increase locally when neural activity increases (specifically, when synaptic input increases). This blood flow response is relatively slow, unfolding over seconds. Here is an example from Russ's [Handbook of fMRI Data Analysis](https://www.cambridge.org/core/books/handbook-of-functional-mri-data-analysis/8EDF966C65811FCCC306F7C916228529):\n",
    "\n",
    "> An example of the hemodynamic responses evoked in area V1 by a contrast-reversing checkerboard displayed for 500 ms. The four different lines are data from four different individuals, showing how variable these responses can be across people. The MRI signal was measured every 250 ms, which accounts for the noisiness of the plots. (Data courtesy of Stephen Engel, University of Minnesota)\n",
    "\n",
    "\n",
    "![HRF Example](./images/hrf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d04b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve with hemodynamic response\n",
    "\n",
    "# boxcar with 100 ms resolution\n",
    "timepoints = np.arange(0, 30, 0.1)\n",
    "\n",
    "boxcar = np.zeros(len(timepoints))  # 30 second window\n",
    "boxcar[10:15] = 1  # 500 ms stimulus starting at 1 second\n",
    "plt.plot(timepoints, boxcar)\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "\n",
    "# magnify the convolved response by 10 for display purposes\n",
    "convolved_resp = np.convolve(boxcar, spm_hrf(0.1), mode=\"full\")[: len(timepoints)] * 10\n",
    "plt.plot(timepoints, convolved_resp)\n",
    "plt.legend([\"stimulus boxcar\", \"convolved response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433750d",
   "metadata": {},
   "source": [
    "Because of this hemodynamic delay, we don't expect the BOLD response to stimuli to follow a boxcar function. Rather, we expect that they will be lagged and smeared. Fortunately, there is good evidence to think that the BOLD response is relatively linear (at least for stimul in the range of 0.5-2 secs), so we can simply convolve our boxcar function with a function that represents the shape of the hemodynamic response. One commonly used function is known as the SPM HRF since it is used in the popular SPM software package. It is a combination of two gamma functions, one that models the positive response and a second that models a slower negative response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(frame_times, desmat[run_num])\n",
    "_ = plt.plot(frame_times, desmat_conv[run_num])\n",
    "plt.xlabel(\"Time (secs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6b15e",
   "metadata": {},
   "source": [
    "### FMRI data preprocessing\n",
    "\n",
    "We now wish to fit the GLM to data from each run. However, before the fMRI data can be analyzed, they need to be preprocessed to address several issues:\n",
    "- We need to estimate and correct for head motion across each scan\n",
    "- We need to estimate a number of other potentia confounding signals, such as physiological fluctuations that relate to breathing or heartbeat, or residual effects of head motion.\n",
    "- We often want to align each individual's brain with a common template, so that data can be combined across individuals.\n",
    "\n",
    "To accomplish this we use the [fMRIPrep](https://fmriprep.org/en/stable/) preprocessing workflow:\n",
    "\n",
    "![fMRIPrep workflow](./images/fmriprep.jpg)\n",
    "\n",
    "fMRIPrep stores its outputs (known generically as derivatives) in a BIDS format that can also be queried using PyBIDS. Lets find the preprocessed BOLD data (which have been transformed into a common template space known as MNI152NLin2009cAsym), and also find the mask images that tell us for each image which voxels fall within the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the brain mask, adding the 'res' entity\n",
    "maskfiles = layout.get(\n",
    "    subject=1,\n",
    "    desc=\"brain\",\n",
    "    space=\"MNI152NLin2009cAsym\",\n",
    "    res=\"2\",\n",
    "    suffix=\"mask\",\n",
    "    extension=\"nii.gz\",\n",
    "    return_type=\"file\",\n",
    ")\n",
    "\n",
    "# Check if any mask files were found\n",
    "if not maskfiles:\n",
    "    raise ValueError(\"No mask files found with the specified criteria.\")\n",
    "\n",
    "print(f\"Found {len(maskfiles)} mask file(s)\")\n",
    "for mask in maskfiles:\n",
    "    print(mask)\n",
    "\n",
    "# Create intersection mask\n",
    "all_masks = image.concat_imgs(maskfiles)\n",
    "mask = image.threshold_img(\n",
    "    image.mean_img(all_masks, copy_header=True), 0.5, copy_header=True\n",
    ")\n",
    "\n",
    "# Get the preprocessed BOLD images, adding the 'res' entity\n",
    "boldfiles = layout.get(\n",
    "    subject=1,\n",
    "    desc=\"preproc\",\n",
    "    space=\"MNI152NLin2009cAsym\",\n",
    "    res=\"2\",\n",
    "    suffix=\"bold\",\n",
    "    extension=\"nii.gz\",\n",
    "    return_type=\"file\",\n",
    ")\n",
    "\n",
    "# Check if any bold files were found\n",
    "if not boldfiles:\n",
    "    raise ValueError(\"No BOLD files found with the specified criteria.\")\n",
    "\n",
    "print(f\"Found {len(boldfiles)} bold file(s)\")\n",
    "for bold in boldfiles:\n",
    "    print(bold)\n",
    "\n",
    "# Create a mean BOLD image\n",
    "meanbold = image.mean_img(boldfiles, copy_header=True)\n",
    "print(\"\\nSuccessfully found and processed all files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194154d",
   "metadata": {},
   "source": [
    "Overlay the common mask across runs on top of the subject's anatomical image, this lets you see the areas where fMRI data are missing due to \"dropout\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d44425",
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_anat_imgs = layout.get(\n",
    "    subject=\"1\",\n",
    "    desc=\"preproc\",\n",
    "    space=\"MNI152NLin2009cAsym\",\n",
    "    res=\"2\",\n",
    "    datatype=\"anat\",\n",
    "    suffix=\"T1w\",\n",
    "    extension=\"nii.gz\",\n",
    "    return_type=\"file\",\n",
    ")\n",
    "\n",
    "print(\"Found the following MNI-coregistered anatomical images for sub-1:\")\n",
    "for img in mni_anat_imgs:\n",
    "    print(img)\n",
    "\n",
    "bg_image = mni_anat_imgs[0]\n",
    "\n",
    "plotting.plot_roi(mask, bg_img=bg_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b98a4e",
   "metadata": {},
   "source": [
    "Load the data from the in-mask voxels for each run, storing them to a dictionary indexed by runs. Since processed in the data in 3D is inconvenient, we will use the Nilearn NiftiMasker to extract the data from all of the in-mask voxels and return them as a 2D (timepoints X voxels) matrix. We will also load and store a set of confound regressors that are computed by fMRIPrep; these include several estimates of head motion, along with the first 8 principal components of signal from within regions not expected to show BOLD activation, such as the ventricles. The NiftiMasker can then remove these confounds from the data.\n",
    "\n",
    "Confounds include the following:\n",
    "- framewise displacement (total motion)\n",
    "- acompcor (principal components of signals from nuisance areas)\n",
    "- estimated motion params (translation and rotation and their derivatives)\n",
    "- cosine bases (modeling low-frequency trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d067818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create both full and deconfounded versions of each dataset\n",
    "bolddata = {}\n",
    "bolddata_deconfounded = {}\n",
    "confounds = {}\n",
    "\n",
    "masker = maskers.NiftiMasker(mask)\n",
    "\n",
    "confound_vars = [\"framewise_displacement\"] + [f\"a_comp_cor_0{i}\" for i in range(8)]\n",
    "confound_vars += [f\"trans_{d}\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"rot_{d}\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"trans_{d}_derivative1\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"rot_{d}_derivative1\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"cosine0{i}\" for i in range(3)]\n",
    "\n",
    "for run_num, X in desmat_conv.items():\n",
    "    print(f\"Loading data for {run_num}...\")\n",
    "\n",
    "    confound_file = layout.get(\n",
    "        subject=1,\n",
    "        run=int(run_num.split(\"-\")[1]),\n",
    "        extension=\"tsv\",\n",
    "        datatype=\"func\",\n",
    "        suffix=\"timeseries\",\n",
    "        return_type=\"filename\",\n",
    "    )[0]\n",
    "\n",
    "    full_confounds = pd.read_csv(confound_file, sep=\"\\t\")\n",
    "    confounds[run_num] = full_confounds[confound_vars].fillna(0)\n",
    "\n",
    "    boldfile = layout.get(\n",
    "        subject=1,\n",
    "        run=int(run_num.split(\"-\")[1]),\n",
    "        extension=\"nii.gz\",\n",
    "        datatype=\"func\",\n",
    "        suffix=\"bold\",\n",
    "        return_type=\"filename\",\n",
    "    )[0]\n",
    "    bolddata[run_num] = masker.fit_transform(boldfile)\n",
    "    bolddata_deconfounded[run_num] = masker.fit_transform(\n",
    "        boldfile, confounds=confounds[run_num]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b98437",
   "metadata": {},
   "source": [
    "Combine the data across runs and fit the statistical model, with a separate intercept for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "bolddata_concat = None\n",
    "bolddata_deconfounded_concat = None\n",
    "\n",
    "X_concat = None\n",
    "\n",
    "run_idx = 0\n",
    "for run_num, rundata in bolddata.items():\n",
    "    if bolddata_concat is None:\n",
    "        bolddata_concat = bolddata[run_num]\n",
    "        bolddata_deconfounded_concat = bolddata_deconfounded[run_num]\n",
    "        X_concat = np.concatenate((desmat_conv[run_num], confounds[run_num]), axis=1)\n",
    "    else:\n",
    "        bolddata_deconfounded_concat = np.concatenate(\n",
    "            (bolddata_deconfounded_concat, bolddata_deconfounded[run_num])\n",
    "        )\n",
    "        bolddata_concat = np.concatenate((bolddata_concat, bolddata[run_num]))\n",
    "        X_concat = np.concatenate(\n",
    "            (\n",
    "                X_concat,\n",
    "                np.concatenate((desmat_conv[run_num], confounds[run_num]), axis=1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# make intercepts for each run\n",
    "nruns = len(bolddata)\n",
    "intercept_mtx = None\n",
    "\n",
    "for i in range(nruns):\n",
    "    run_intercept = np.zeros((n_timepoints, nruns))\n",
    "    run_intercept[:, i] = 1\n",
    "    if intercept_mtx is None:\n",
    "        intercept_mtx = run_intercept\n",
    "    else:\n",
    "        intercept_mtx = np.concatenate((intercept_mtx, run_intercept))\n",
    "\n",
    "X_concat = np.append(X_concat, intercept_mtx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_concat, aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0992de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_concat, bolddata_concat)\n",
    "\n",
    "# compute t/p values for each of the condition regressors\n",
    "# from https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "\n",
    "sse = np.sum((lm.predict(X_concat) - bolddata_concat) ** 2, axis=0) / float(\n",
    "    X_concat.shape[0] - X_concat.shape[1]\n",
    ")\n",
    "se = np.array(\n",
    "    [\n",
    "        np.sqrt(np.diagonal(sse[i] * np.linalg.inv(np.dot(X_concat.T, X_concat))))\n",
    "        for i in range(sse.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "t = lm.coef_ / se\n",
    "p = 2 * (\n",
    "    1 - scipy.stats.t.cdf(np.abs(t), bolddata[run_num].shape[0] - X_concat.shape[1])\n",
    ")\n",
    "\n",
    "# compute r-squared for the full model\n",
    "rsquared_full = sklearn.metrics.r2_score(\n",
    "    bolddata_concat, lm.predict(X_concat), multioutput=\"raw_values\"\n",
    ")\n",
    "\n",
    "# stored the fitted response\n",
    "fitted_resp_full = lm.predict(X_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13717f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also fit the model for the confounds only and compute r-squared for each voxel\n",
    "\n",
    "X_confound = X_concat[:, 8:]\n",
    "lm.fit(X_confound, bolddata_concat)\n",
    "rsquared_confound = sklearn.metrics.r2_score(\n",
    "    bolddata_concat, lm.predict(X_confound), multioutput=\"raw_values\"\n",
    ")\n",
    "rsquared_confound_img = masker.inverse_transform(rsquared_confound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce93b79",
   "metadata": {},
   "source": [
    "If we plot the r-squared for the confound model, we see that it accounts for a large amount of variance across much of the brain, especially near the edges, where motion is particularly impactful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    rsquared_confound_img,\n",
    "    bg_image,\n",
    "    title=\"r-squared for confound model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(-20, 10, 5),\n",
    ")\n",
    "plotting.plot_stat_map(\n",
    "    rsquared_confound_img,\n",
    "    bg_image,\n",
    "    title=\"r-squared for confound model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(10, 40, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3477774",
   "metadata": {},
   "source": [
    "To see the added variance accounted for by the task model, we can compute the incremental r-squared, which we see is much more isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_rsquared_img = masker.inverse_transform(rsquared_full - rsquared_confound)\n",
    "\n",
    "plotting.plot_stat_map(\n",
    "    incremental_rsquared_img,\n",
    "    bg_image,\n",
    "    title=\"delta r-squared for full model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(-20, 10, 5),\n",
    ")\n",
    "plotting.plot_stat_map(\n",
    "    incremental_rsquared_img,\n",
    "    bg_image,\n",
    "    title=\"delta r-squared for full model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(10, 40, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c72f4f",
   "metadata": {},
   "source": [
    "We can also visualize the statistical maps associated with each condition. We will use the false discovery rate as a rough control for Type I error across the brain, but note that concerns have been raised about the use of FDR in spatially autocorrelated maps like fMRI data ([Chumbley & Friston, 2009](https://pubmed.ncbi.nlm.nih.gov/18603449/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a83766",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmaps = {}\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "# convert the 8 condition images back to nifti\n",
    "for i, cond in enumerate(conditions):\n",
    "    tmaps[cond] = glm.threshold_stats_img(\n",
    "        masker.inverse_transform(t[:, i]),\n",
    "        alpha=alpha,\n",
    "        cluster_threshold=20,\n",
    "        height_control=\"fdr\",\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the images\n",
    "if not os.path.exists(\"tmaps\"):\n",
    "    os.mkdir(\"tmaps\")\n",
    "\n",
    "for i, cond in enumerate(conditions):\n",
    "    plotting.plot_stat_map(\n",
    "        tmaps[cond],\n",
    "        bg_image,\n",
    "        threshold=3.0,\n",
    "        title=cond,\n",
    "        display_mode=\"z\",\n",
    "        cut_coords=np.arange(-20, 10, 5),\n",
    "    )\n",
    "    tmaps[cond].to_filename(f\"tmaps/{cond}_tmap.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5e022",
   "metadata": {},
   "source": [
    "Examine the timeseries from a region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find highest activating voxel for faces\n",
    "face_regressor_idx = np.where(conditions == \"face\")[0][0]\n",
    "facevox = np.argmax(t[:, face_regressor_idx])\n",
    "\n",
    "plt.plot(bolddata_concat[:200, facevox])\n",
    "plt.plot(fitted_resp_full[:200, facevox])\n",
    "plt.legend([\"data\", \"fitted response\"])\n",
    "plt.title(f\"Face voxel (r-squared = {rsquared_full[facevox]:0.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find highest activating voxel for houses\n",
    "house_regressor_idx = np.where(conditions == \"house\")[0][0]\n",
    "housevox = np.argmax(t[:, house_regressor_idx])\n",
    "\n",
    "plt.plot(bolddata_concat[:200, housevox])\n",
    "plt.plot(fitted_resp_full[:200, housevox])\n",
    "plt.legend([\"data\", \"fitted response\"])\n",
    "plt.title(f\"Face voxel (r-squared = {rsquared_full[housevox]:0.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
