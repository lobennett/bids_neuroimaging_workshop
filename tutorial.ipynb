{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e90363",
   "metadata": {},
   "source": [
    "**For cloud environments (Google Colab, Binder, etc.):** Run the following cells to install dependencies and download the neuroimaging dataset from OpenNeuro.\n",
    "\n",
    "```python\n",
    "# Install dependencies\n",
    "!pip3 install -r https://raw.githubusercontent.com/lobennett/bids_neuroimaging_workshop/main/requirements.txt\n",
    "\n",
    "# Download dataset (Haxby et al., 2001 - face and object recognition)\n",
    "!wget https://raw.githubusercontent.com/lobennett/bids_neuroimaging_workshop/main/download.sh\n",
    "!bash download.sh\n",
    "```\n",
    "\n",
    "**Note:** The download may take several minutes as the dataset is approximately 2GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1jidder3lik",
   "metadata": {},
   "source": [
    "### Import libraries and define utility functions\n",
    "\n",
    "This cell imports all necessary Python libraries for neuroimaging analysis and defines a custom SPM hemodynamic response function (HRF) that models the brain's blood oxygen response to neural activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "from nilearn import plotting, maskers, glm, image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from bids import BIDSLayout\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn.glm.first_level import hemodynamic_models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def spm_hrf(TR, p=[6, 16, 1, 1, 6, 0, 32]):\n",
    "    \"\"\"An implementation of spm_hrf.m from the SPM distribution\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Required:\n",
    "    TR: repetition time at which to generate the HRF (in seconds)\n",
    "\n",
    "    Optional:\n",
    "    p: list with parameters of the two gamma functions:\n",
    "                                                        defaults\n",
    "                                                        (seconds)\n",
    "        p[0] - delay of response (relative to onset)      6\n",
    "        p[1] - delay of undershoot (relative to onset)    16\n",
    "        p[2] - dispersion of response                     1\n",
    "        p[3] - dispersion of undershoot                   1\n",
    "        p[4] - ratio of response to undershoot            6\n",
    "        p[5] - onset (seconds)                            0\n",
    "        p[6] - length of kernel (seconds)                 32\n",
    "\n",
    "    \"\"\"\n",
    "    import scipy.stats\n",
    "\n",
    "    p = [float(x) for x in p]\n",
    "\n",
    "    fMRI_T = 16.0\n",
    "\n",
    "    TR = float(TR)\n",
    "    dt = TR / fMRI_T\n",
    "    u = np.arange(p[6] / dt + 1) - p[5] / dt\n",
    "    hrf = (\n",
    "        scipy.stats.gamma.pdf(u, p[0] / p[2], scale=1.0 / (dt / p[2]))\n",
    "        - scipy.stats.gamma.pdf(u, p[1] / p[3], scale=1.0 / (dt / p[3])) / p[4]\n",
    "    )\n",
    "    good_pts = np.array(range(int(p[6] / TR))) * fMRI_T\n",
    "    hrf = hrf[list(good_pts.astype(int))]\n",
    "    hrf = hrf / np.sum(hrf)\n",
    "    return hrf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000cf5",
   "metadata": {},
   "source": [
    "### Data organization\n",
    "\n",
    "The data are stored using a framework called the [Brain Imaging Data Structure](https://bids.neuroimaging.io/) (BIDS), which provides a standard way to organize neuroimaging data. These data can then be queried using the [PyBIDS](https://pypi.org/project/pybids/) Python package.\n",
    "\n",
    "We will use the dataset from Haxby et al., 2001, which is available from OpenNeuro [ds000105](https://openneuro.org/datasets/ds000105/versions/3.0.0). Here are details about the data (from [PvMVPA](http://www.pymvpa.org/datadb/haxby2001.html)):\n",
    "\n",
    ">This is a block-design fMRI dataset from a study on face and object representation in human ventral temporal cortex. It consists of 6 subjects with 12 runs per subject. In each run, the subjects passively viewed greyscale images of eight object categories, grouped in 24s blocks separated by rest periods. Each image was shown for 500ms and was followed by a 1500ms inter-stimulus interval. Full-brain fMRI data were recorded with a volume repetition time of 2.5s, thus, a stimulus block was covered by roughly 9 volumes. This dataset has been repeatedly reanalyzed. For a complete description of the experimental design, fMRI acquisition parameters, and previously obtained results see the references below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9llc8vo4kna",
   "metadata": {},
   "source": [
    "### Setup BIDS data layout\n",
    "\n",
    "Initialize the BIDS (Brain Imaging Data Structure) layout for accessing neuroimaging data. This checks for mounted data at `/mnt/bids/` first, otherwise uses the local dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e959c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data in PyBIDS layout\n",
    "# Check if data is mounted at /mnt/bids/, otherwise use local data\n",
    "if Path(\"/mnt/bids/\").exists():\n",
    "    bids_dir = Path(\"/mnt/bids/\")\n",
    "    print(\"Using mounted data at /mnt/bids/\")\n",
    "else:\n",
    "    bids_dir = Path(\"./ds000105\")\n",
    "    print(\"Using local data at ./ds000105\")\n",
    "\n",
    "layout = BIDSLayout(bids_dir, derivatives=True)\n",
    "print(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28kmetox9s",
   "metadata": {},
   "source": [
    "### Query and examine anatomical images\n",
    "\n",
    "Load the structural T1-weighted brain images for subject 1 and examine the NIfTI header information to understand the image properties and acquisition parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c27a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_imgs = layout.get(\n",
    "    subject=1, extension=\"nii.gz\", suffix=\"T1w\", return_type=\"filename\"\n",
    ")\n",
    "\n",
    "print(\"Found the following anatomical images for sub-1:\")\n",
    "for img in anat_imgs:\n",
    "    print(img)\n",
    "\n",
    "# Now let's load the anatomical image and display the header\n",
    "anat_img_nib = nib.load(anat_imgs[1])\n",
    "print(\"The first anatomical image has the following header information:\\n\")\n",
    "print(anat_img_nib.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba640d90",
   "metadata": {},
   "source": [
    "These data are stored in a common image format known as [NifTI](https://nifti.nimh.nih.gov/nifti-1/), which is the main standard used for neuroimaging data in our field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h69hpcjunlf",
   "metadata": {},
   "source": [
    "### Display anatomical brain image\n",
    "\n",
    "Visualize the structural brain image using Nilearn's plotting functions to see the anatomy across different brain slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9df66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vmax to 200 to provide better contrast\n",
    "plotting.plot_anat(anat_imgs[1], vmax=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "niwm8cgj3g",
   "metadata": {},
   "source": [
    "### Query and examine functional images\n",
    "\n",
    "Load the 4D functional BOLD (Blood Oxygen Level Dependent) images for subject 1 and examine their dimensions. These images contain the brain activity data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_imgs = layout.get(\n",
    "    subject=1, extension=\"nii.gz\", suffix=\"bold\", return_type=\"filename\"\n",
    ")\n",
    "\n",
    "print(\"Found the following functional images for sub-1:\")\n",
    "for img in func_imgs:\n",
    "    print(img)\n",
    "\n",
    "# These images are 4-dimensional.\n",
    "# We can load their data using the nibabel package (https://pypi.org/project/nibabel/) and inspect its contents:\n",
    "func_img = nib.load(func_imgs[0])\n",
    "func_data = func_img.get_fdata()\n",
    "\n",
    "print(f\"Functional image has shape: {func_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdnodfffrn",
   "metadata": {},
   "source": [
    "### Display functional image\n",
    "\n",
    "Visualize a single timepoint from the functional data to see the brain anatomy and signal quality from the BOLD acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(image.index_img(func_imgs[0], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a4c22",
   "metadata": {},
   "source": [
    "### Task information\n",
    "In order to analyze the fMRI data we need to also load the information regarding which stimuli were presented at which times. These are stored according to the BIDS standard in an \"events.tsv\" associated with each task run. The standard format has three columns that specify the onset of each event (in seconds from the beginning of the run), the duration of the event (in seconds), and a variable called trial_type that specifies the experimental condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7343fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = layout.get(\n",
    "    subject=1, extension=\"tsv\", suffix=\"events\", return_type=\"filename\"\n",
    ")\n",
    "\n",
    "events = {}\n",
    "for f in event_files:\n",
    "    run_num = f.split(\"_\")[-2]\n",
    "    events[run_num] = pd.read_csv(f, sep=\"\\t\")\n",
    "\n",
    "print(f\"Found {len(events)} event files\")\n",
    "events[run_num].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gtzzxm0r0ue",
   "metadata": {},
   "source": [
    "### Display experimental conditions\n",
    "\n",
    "Show the different object categories that were presented to the subject during the experiment (faces, objects, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db470739",
   "metadata": {},
   "source": [
    "There should be 8 different object types presented in each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = events[run_num].trial_type.unique()\n",
    "print(conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa6x1q4i",
   "metadata": {},
   "source": [
    "### Extract timing parameters\n",
    "\n",
    "Get the number of timepoints in the functional data and the repetition time (TR) - the time between successive brain volume acquisitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about timeseries length\n",
    "n_timepoints = func_img.dataobj.shape[3]\n",
    "print(n_timepoints, \"timepoints\")\n",
    "\n",
    "# Get the length of each image acquisition, known as repetition time or TR\n",
    "TR = func_img.header.get_zooms()[3]\n",
    "print(\"TR:\", TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbk32mi5ek",
   "metadata": {},
   "source": [
    "### Create design matrices\n",
    "\n",
    "Build design matrices for the general linear model (GLM). Create both boxcar (unconvolved) and hemodynamically convolved versions of the regressors for each experimental condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "desmat = {}\n",
    "desmat_conv = {}\n",
    "\n",
    "# onset times for each timepoint, in seconds\n",
    "frame_times = np.arange(0, n_timepoints * TR, TR)\n",
    "\n",
    "# length of each task block, in seconds\n",
    "block_length = 24\n",
    "\n",
    "for run_num, event_df in events.items():\n",
    "    # first create empty design matrices, to be filled below\n",
    "    desmat[run_num] = np.zeros((n_timepoints, len(conditions)))\n",
    "    desmat_conv[run_num] = np.zeros(desmat[run_num].shape)\n",
    "\n",
    "    for i, cond in enumerate(conditions):\n",
    "        # create a single block instead of using individual events\n",
    "        cond_events = events[run_num].query(f'trial_type == \"{cond}\"')[\n",
    "            [\"onset\", \"duration\"]\n",
    "        ]\n",
    "        cond_events[\"amplitude\"] = 1\n",
    "\n",
    "        # set the first event to the block length, and then remove the rest\n",
    "        cond_events.iloc[0, 1] = block_length\n",
    "        cond_events = cond_events.iloc[0, :]\n",
    "\n",
    "        # create the unconvolved design matrix\n",
    "        desmat[run_num][:, i] = hemodynamic_models.compute_regressor(\n",
    "            cond_events.to_numpy()[:, np.newaxis], None, frame_times, oversampling=50\n",
    "        )[0][:, 0]\n",
    "\n",
    "        # create the design matrix convolved with the SPM hemodynamic response\n",
    "        desmat_conv[run_num][:, i] = hemodynamic_models.compute_regressor(\n",
    "            cond_events.to_numpy()[:, np.newaxis], \"spm\", frame_times, oversampling=50\n",
    "        )[0][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1orhtlbbe2u",
   "metadata": {},
   "source": [
    "### Visualize design matrix as image\n",
    "\n",
    "It is customary to display the design matrix as an image. In this case, white blocks represent active blocks for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(desmat[run_num], aspect=\"auto\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf50865d",
   "metadata": {},
   "source": [
    "We can also plot these as timeseries, which shows that each has a boxcar form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(frame_times, desmat[run_num])\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.legend(conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l1z9cy2qxjb",
   "metadata": {},
   "source": [
    "### Plot design matrix timeseries\n",
    "\n",
    "Show the boxcar design matrix as individual timeseries to visualize the timing of each experimental condition over the course of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f6cce",
   "metadata": {},
   "source": [
    "### Hemodynamic convolution\n",
    "\n",
    "The standard fMRI technique (known as Blood Oxygen Level Dependent or BOLD fMRI) measures changes in MRI signal related to changes in blood oxygen levels, which increase locally when neural activity increases (specifically, when synaptic input increases). This blood flow response is relatively slow, unfolding over seconds. Here is an example from Russ's [Handbook of fMRI Data Analysis](https://www.cambridge.org/core/books/handbook-of-functional-mri-data-analysis/8EDF966C65811FCCC306F7C916228529):\n",
    "\n",
    "> An example of the hemodynamic responses evoked in area V1 by a contrast-reversing checkerboard displayed for 500 ms. The four different lines are data from four different individuals, showing how variable these responses can be across people. The MRI signal was measured every 250 ms, which accounts for the noisiness of the plots. (Data courtesy of Stephen Engel, University of Minnesota)\n",
    "\n",
    "\n",
    "![HRF Example](./images/hrf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qpqzlvecg99",
   "metadata": {},
   "source": [
    "### View hemodynamic response convolution\n",
    "\n",
    "Show how a brief stimulus boxcar is convolved with the hemodynamic response function to create the expected BOLD response shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d04b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve with hemodynamic response\n",
    "\n",
    "# boxcar with 100 ms resolution\n",
    "timepoints = np.arange(0, 30, 0.1)\n",
    "\n",
    "boxcar = np.zeros(len(timepoints))  # 30 second window\n",
    "boxcar[10:15] = 1  # 500 ms stimulus starting at 1 second\n",
    "plt.plot(timepoints, boxcar)\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "\n",
    "# magnify the convolved response by 10 for display purposes\n",
    "convolved_resp = np.convolve(boxcar, spm_hrf(0.1), mode=\"full\")[: len(timepoints)] * 10\n",
    "plt.plot(timepoints, convolved_resp)\n",
    "plt.legend([\"stimulus boxcar\", \"convolved response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j5tu4edxm7n",
   "metadata": {},
   "source": [
    "### Compare boxcar vs convolved design matrices\n",
    "\n",
    "Overlay the original boxcar design matrix with the hemodynamically convolved version to show how the HRF transforms the expected BOLD signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433750d",
   "metadata": {},
   "source": [
    "Because of this hemodynamic delay, we don't expect the BOLD response to stimuli to follow a boxcar function. Rather, we expect that they will be lagged and smeared. Fortunately, there is good evidence to think that the BOLD response is relatively linear (at least for stimul in the range of 0.5-2 secs), so we can simply convolve our boxcar function with a function that represents the shape of the hemodynamic response. One commonly used function is known as the SPM HRF since it is used in the popular SPM software package. It is a combination of two gamma functions, one that models the positive response and a second that models a slower negative response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(frame_times, desmat[run_num])\n",
    "_ = plt.plot(frame_times, desmat_conv[run_num])\n",
    "plt.xlabel(\"Time (secs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6b15e",
   "metadata": {},
   "source": [
    "### FMRI data preprocessing\n",
    "\n",
    "We now wish to fit the GLM to data from each run. However, before the fMRI data can be analyzed, they need to be preprocessed to address several issues:\n",
    "- We need to estimate and correct for head motion across each scan\n",
    "- We need to estimate a number of other potential confounding signals, such as physiological fluctuations that relate to breathing or heartbeat, or residual effects of head motion.\n",
    "- We often want to align each individual's brain with a common template, so that data can be combined across individuals.\n",
    "\n",
    "To accomplish this we use the [fMRIPrep](https://fmriprep.org/en/stable/) preprocessing workflow:\n",
    "\n",
    "![fMRIPrep workflow](./images/fmriprep.jpg)\n",
    "\n",
    "fMRIPrep stores its outputs (known generically as derivatives) in a BIDS format that can also be queried using PyBIDS. Lets find the preprocessed BOLD data (which have been transformed into a common template space known as MNI152NLin2009cAsym), and also find the mask images that tell us for each image which voxels fall within the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63in1q0pvu6",
   "metadata": {},
   "source": [
    "### Load preprocessed BOLD data and brain masks\n",
    "\n",
    "Access the fMRIPrep-processed functional data that has been aligned to standard MNI space and create brain masks to define valid voxels for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the brain mask, adding the 'res' entity\n",
    "maskfiles = layout.get(\n",
    "    subject=1,\n",
    "    desc=\"brain\",\n",
    "    space=\"MNI152NLin2009cAsym\",\n",
    "    res=\"2\",\n",
    "    suffix=\"mask\",\n",
    "    extension=\"nii.gz\",\n",
    "    return_type=\"file\",\n",
    ")\n",
    "\n",
    "# Check if any mask files were found\n",
    "if not maskfiles:\n",
    "    raise ValueError(\"No mask files found with the specified criteria.\")\n",
    "\n",
    "print(f\"Found {len(maskfiles)} mask file(s)\")\n",
    "for mask in maskfiles:\n",
    "    print(mask)\n",
    "\n",
    "# Create intersection mask\n",
    "all_masks = image.concat_imgs(maskfiles)\n",
    "mask = image.threshold_img(\n",
    "    image.mean_img(all_masks, copy_header=True), 0.5, copy_header=True\n",
    ")\n",
    "\n",
    "# Get the preprocessed BOLD images, adding the 'res' entity\n",
    "boldfiles = layout.get(\n",
    "    subject=1,\n",
    "    desc=\"preproc\",\n",
    "    space=\"MNI152NLin2009cAsym\",\n",
    "    res=\"2\",\n",
    "    suffix=\"bold\",\n",
    "    extension=\"nii.gz\",\n",
    "    return_type=\"file\",\n",
    ")\n",
    "\n",
    "# Check if any bold files were found\n",
    "if not boldfiles:\n",
    "    raise ValueError(\"No BOLD files found with the specified criteria.\")\n",
    "\n",
    "print(f\"Found {len(boldfiles)} bold file(s)\")\n",
    "for bold in boldfiles:\n",
    "    print(bold)\n",
    "\n",
    "# Create a mean BOLD image\n",
    "meanbold = image.mean_img(boldfiles, copy_header=True)\n",
    "print(\"\\nSuccessfully found and processed all files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ywwin0f902h",
   "metadata": {},
   "source": [
    "### Visualize brain mask coverage\n",
    "\n",
    "Display the brain mask overlaid on the anatomical image to show areas where functional data is available and identify potential signal dropout regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d44425",
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_anat_imgs = layout.get(\n",
    "    subject=\"1\",\n",
    "    desc=\"preproc\",\n",
    "    space=\"MNI152NLin2009cAsym\",\n",
    "    res=\"2\",\n",
    "    datatype=\"anat\",\n",
    "    suffix=\"T1w\",\n",
    "    extension=\"nii.gz\",\n",
    "    return_type=\"file\",\n",
    ")\n",
    "\n",
    "print(\"Found the following MNI-coregistered anatomical images for sub-1:\")\n",
    "for img in mni_anat_imgs:\n",
    "    print(img)\n",
    "\n",
    "bg_image = mni_anat_imgs[0]\n",
    "\n",
    "plotting.plot_roi(mask, bg_img=bg_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddlggwpw5sp",
   "metadata": {},
   "source": [
    "### Extract BOLD timeseries with confound regression\n",
    "\n",
    "Use NiftiMasker to extract BOLD timeseries data from brain voxels, loading both raw and deconfounded versions. Confounds include motion parameters and physiological noise estimates from fMRIPrep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b98a4e",
   "metadata": {},
   "source": [
    "Load the data from the in-mask voxels for each run, storing them to a dictionary indexed by runs. Since processed in the data in 3D is inconvenient, we will use the Nilearn NiftiMasker to extract the data from all of the in-mask voxels and return them as a 2D (timepoints X voxels) matrix. We will also load and store a set of confound regressors that are computed by fMRIPrep; these include several estimates of head motion, along with the first 8 principal components of signal from within regions not expected to show BOLD activation, such as the ventricles. The NiftiMasker can then remove these confounds from the data.\n",
    "\n",
    "Confounds include the following:\n",
    "- framewise displacement (total motion)\n",
    "- acompcor (principal components of signals from nuisance areas)\n",
    "- estimated motion params (translation and rotation and their derivatives)\n",
    "- cosine bases (modeling low-frequency trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d067818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create both full and deconfounded versions of each dataset\n",
    "bolddata = {}\n",
    "bolddata_deconfounded = {}\n",
    "confounds = {}\n",
    "\n",
    "masker = maskers.NiftiMasker(mask)\n",
    "\n",
    "confound_vars = [\"framewise_displacement\"] + [f\"a_comp_cor_0{i}\" for i in range(8)]\n",
    "confound_vars += [f\"trans_{d}\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"rot_{d}\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"trans_{d}_derivative1\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"rot_{d}_derivative1\" for d in [\"x\", \"y\", \"z\"]]\n",
    "confound_vars += [f\"cosine0{i}\" for i in range(3)]\n",
    "\n",
    "for run_num, X in desmat_conv.items():\n",
    "    print(f\"Loading data for {run_num}...\")\n",
    "\n",
    "    confound_file = layout.get(\n",
    "        subject=1,\n",
    "        run=int(run_num.split(\"-\")[1]),\n",
    "        extension=\"tsv\",\n",
    "        datatype=\"func\",\n",
    "        suffix=\"timeseries\",\n",
    "        return_type=\"filename\",\n",
    "    )[0]\n",
    "\n",
    "    full_confounds = pd.read_csv(confound_file, sep=\"\\t\")\n",
    "    confounds[run_num] = full_confounds[confound_vars].fillna(0)\n",
    "\n",
    "    boldfile = layout.get(\n",
    "        subject=1,\n",
    "        run=int(run_num.split(\"-\")[1]),\n",
    "        extension=\"nii.gz\",\n",
    "        datatype=\"func\",\n",
    "        suffix=\"bold\",\n",
    "        return_type=\"filename\",\n",
    "    )[0]\n",
    "    bolddata[run_num] = masker.fit_transform(boldfile)\n",
    "    bolddata_deconfounded[run_num] = masker.fit_transform(\n",
    "        boldfile, confounds=confounds[run_num]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w7orke4yw6",
   "metadata": {},
   "source": [
    "### Concatenate data across runs\n",
    "\n",
    "Combine BOLD data from all runs into single matrices and create the full design matrix including task regressors, confounds, and run-specific intercepts for GLM fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "bolddata_concat = None\n",
    "bolddata_deconfounded_concat = None\n",
    "\n",
    "X_concat = None\n",
    "\n",
    "run_idx = 0\n",
    "for run_num, rundata in bolddata.items():\n",
    "    if bolddata_concat is None:\n",
    "        bolddata_concat = bolddata[run_num]\n",
    "        bolddata_deconfounded_concat = bolddata_deconfounded[run_num]\n",
    "        X_concat = np.concatenate((desmat_conv[run_num], confounds[run_num]), axis=1)\n",
    "    else:\n",
    "        bolddata_deconfounded_concat = np.concatenate(\n",
    "            (bolddata_deconfounded_concat, bolddata_deconfounded[run_num])\n",
    "        )\n",
    "        bolddata_concat = np.concatenate((bolddata_concat, bolddata[run_num]))\n",
    "        X_concat = np.concatenate(\n",
    "            (\n",
    "                X_concat,\n",
    "                np.concatenate((desmat_conv[run_num], confounds[run_num]), axis=1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# make intercepts for each run\n",
    "nruns = len(bolddata)\n",
    "intercept_mtx = None\n",
    "\n",
    "for i in range(nruns):\n",
    "    run_intercept = np.zeros((n_timepoints, nruns))\n",
    "    run_intercept[:, i] = 1\n",
    "    if intercept_mtx is None:\n",
    "        intercept_mtx = run_intercept\n",
    "    else:\n",
    "        intercept_mtx = np.concatenate((intercept_mtx, run_intercept))\n",
    "\n",
    "X_concat = np.append(X_concat, intercept_mtx, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vpssz9bv8ki",
   "metadata": {},
   "source": [
    "### Visualize complete design matrix\n",
    "\n",
    "Display the full design matrix containing task regressors, confound variables, and run-specific intercepts that will be used in the GLM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_concat, aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qalhjb62t2",
   "metadata": {},
   "source": [
    "### Fit general linear model\n",
    "\n",
    "Perform GLM fitting to estimate brain responses to each experimental condition, computing statistical significance and model fit metrics (R-squared) for each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0992de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_concat, bolddata_concat)\n",
    "\n",
    "# compute t/p values for each of the condition regressors\n",
    "# from https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "\n",
    "sse = np.sum((lm.predict(X_concat) - bolddata_concat) ** 2, axis=0) / float(\n",
    "    X_concat.shape[0] - X_concat.shape[1]\n",
    ")\n",
    "se = np.array(\n",
    "    [\n",
    "        np.sqrt(np.diagonal(sse[i] * np.linalg.inv(np.dot(X_concat.T, X_concat))))\n",
    "        for i in range(sse.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "t = lm.coef_ / se\n",
    "p = 2 * (\n",
    "    1 - scipy.stats.t.cdf(np.abs(t), bolddata[run_num].shape[0] - X_concat.shape[1])\n",
    ")\n",
    "\n",
    "# compute r-squared for the full model\n",
    "rsquared_full = sklearn.metrics.r2_score(\n",
    "    bolddata_concat, lm.predict(X_concat), multioutput=\"raw_values\"\n",
    ")\n",
    "\n",
    "# stored the fitted response\n",
    "fitted_resp_full = lm.predict(X_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blma0l6qbl",
   "metadata": {},
   "source": [
    "### Fit confound-only model\n",
    "\n",
    "Create a baseline model using only confound regressors (no task) to assess how much variance is explained by nuisance factors like motion and physiological noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13717f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also fit the model for the confounds only and compute r-squared for each voxel\n",
    "\n",
    "X_confound = X_concat[:, 8:]\n",
    "lm.fit(X_confound, bolddata_concat)\n",
    "rsquared_confound = sklearn.metrics.r2_score(\n",
    "    bolddata_concat, lm.predict(X_confound), multioutput=\"raw_values\"\n",
    ")\n",
    "rsquared_confound_img = masker.inverse_transform(rsquared_confound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eixjahioto",
   "metadata": {},
   "source": [
    "### Visualize confound model R-squared\n",
    "\n",
    "Display brain maps showing how much variance is explained by confound regressors alone, revealing areas most affected by motion and physiological noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    rsquared_confound_img,\n",
    "    bg_image,\n",
    "    title=\"r-squared for confound model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(-20, 10, 5),\n",
    ")\n",
    "plotting.plot_stat_map(\n",
    "    rsquared_confound_img,\n",
    "    bg_image,\n",
    "    title=\"r-squared for confound model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(10, 40, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9nlgvp2vysr",
   "metadata": {},
   "source": [
    "### Visualize task-specific variance\n",
    "\n",
    "Show the incremental R-squared (additional variance explained by task regressors beyond confounds) to identify brain regions responding specifically to the experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_rsquared_img = masker.inverse_transform(rsquared_full - rsquared_confound)\n",
    "\n",
    "plotting.plot_stat_map(\n",
    "    incremental_rsquared_img,\n",
    "    bg_image,\n",
    "    title=\"delta r-squared for full model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(-20, 10, 5),\n",
    ")\n",
    "plotting.plot_stat_map(\n",
    "    incremental_rsquared_img,\n",
    "    bg_image,\n",
    "    title=\"delta r-squared for full model\",\n",
    "    display_mode=\"z\",\n",
    "    cut_coords=np.arange(10, 40, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uztyg9zbu7b",
   "metadata": {},
   "source": [
    "### Apply statistical thresholding\n",
    "\n",
    "Convert t-statistics to thresholded statistical maps for each experimental condition using false discovery rate (FDR) correction and cluster extent thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a83766",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmaps = {}\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "# convert the 8 condition images back to nifti\n",
    "for i, cond in enumerate(conditions):\n",
    "    tmaps[cond] = glm.threshold_stats_img(\n",
    "        masker.inverse_transform(t[:, i]),\n",
    "        alpha=alpha,\n",
    "        cluster_threshold=20,\n",
    "        height_control=\"fdr\",\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lff5etae2d9",
   "metadata": {},
   "source": [
    "### Display activation maps for each condition\n",
    "\n",
    "Generate and save brain activation maps for each experimental condition, showing where significant BOLD responses occur for different object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the images\n",
    "if not os.path.exists(\"tmaps\"):\n",
    "    os.mkdir(\"tmaps\")\n",
    "\n",
    "for i, cond in enumerate(conditions):\n",
    "    plotting.plot_stat_map(\n",
    "        tmaps[cond],\n",
    "        bg_image,\n",
    "        threshold=3.0,\n",
    "        title=cond,\n",
    "        display_mode=\"z\",\n",
    "        cut_coords=np.arange(-20, 10, 5),\n",
    "    )\n",
    "    tmaps[cond].to_filename(f\"tmaps/{cond}_tmap.nii.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
